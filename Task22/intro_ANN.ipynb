{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Artificial Neural Networks (ANNs)\n",
    "\n",
    "Artificial Neural Networks (ANNs) are computational models inspired by the human brain. They consist of interconnected units called neurons, organized in layers. ANNs are capable of recognizing patterns, making decisions, and learning from data. They are widely used in applications such as image and speech recognition, natural language processing, and more.\n",
    "\n",
    "### Structure of an ANN\n",
    "\n",
    "- **Input Layer**: This layer receives the input data. Each neuron in the input layer represents a feature of the input data. For example, in an image recognition task, each pixel value of an image can be considered a feature.\n",
    "- **Hidden Layers**: These layers perform intermediate computations and transformations. The number of hidden layers and the number of neurons in each layer can vary, impacting the networkâ€™s ability to learn complex patterns.\n",
    "- **Output Layer**: This layer produces the final output of the network, typically representing predictions or classifications. The number of neurons in the output layer usually corresponds to the number of classes in classification tasks or to the dimensionality of the output in regression tasks.\n",
    "- **Neurons and Weights**: Each neuron in a layer is connected to every neuron in the subsequent layer. The connections between neurons have associated weights, which are adjustable parameters that the network learns during training. Each neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function to produce its output.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to model complex relationships. Common activation functions include:\n",
    "\n",
    "- **Sigmoid Function**: Transforms input values to a range from 0 to 1. Commonly used in the output layer of binary classification tasks.\n",
    "- **ReLU (Rectified Linear Unit)**: Passes the input directly if it is positive; otherwise, it outputs zero. Extensively used in hidden layers as it helps to address the vanishing gradient issue.\n",
    "- **Tanh (Hyperbolic Tangent)**: Maps input values to a range from -1 to 1. Often performs better in practice as it outputs both positive and negative values.\n",
    "\n",
    "### Training an ANN\n",
    "\n",
    "Training an ANN involves adjusting the weights and biases to minimize the difference between the predicted outputs and the actual target values. This process is done using a method called backpropagation combined with an optimization algorithm such as gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (2 features and 1 target)\n",
    "inputs = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0], [5.0, 6.0]])\n",
    "targets = torch.tensor([[8.0], [13.0], [18.0], [23.0], [28.0]])  # y = 2*x1 + 3*x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)  \n",
    "        self.fc2 = nn.Linear(3, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation after the first layer\n",
    "        x = self.fc2(x)  \n",
    "        return x\n",
    "    \n",
    "model=SimpleANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam \n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01) # you can also try stochastic gradient decent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 367.9432373046875\n",
      "Epoch 101/1000, Loss: 15.354846000671387\n",
      "Epoch 201/1000, Loss: 0.45917773246765137\n",
      "Epoch 301/1000, Loss: 0.39582639932632446\n",
      "Epoch 401/1000, Loss: 0.3301660120487213\n",
      "Epoch 501/1000, Loss: 0.2668886184692383\n",
      "Epoch 601/1000, Loss: 0.20918579399585724\n",
      "Epoch 701/1000, Loss: 0.15896213054656982\n",
      "Epoch 801/1000, Loss: 0.11706038564443588\n",
      "Epoch 901/1000, Loss: 0.08347444236278534\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    outputs = model(inputs)  # Forward pass: Compute predictions\n",
    "    loss = criterion(outputs, targets)  # Compute the loss\n",
    "    loss.backward()  # Backward pass: Compute gradients\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/1000, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for input [6.0, 7.0]: 32.60346221923828\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with a new input\n",
    "with torch.no_grad():\n",
    "    test_input = torch.tensor([[6.0, 7.0]])  \n",
    "    predicted = model(test_input)\n",
    "    print(f'Predicted value for input [6.0, 7.0]: {predicted.item()}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the simple code for ANN and produce good results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
