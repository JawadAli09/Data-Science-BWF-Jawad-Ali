{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation in Machine Learning\n",
    "\n",
    "Text generation in machine learning is a fascinating area of natural language processing (NLP) where models learn to generate coherent and contextually relevant text. This process involves training models on large corpora of text so that they can predict the next word or sequence of words based on the preceding text. Below is a detailed explanation of the key concepts, followed by a simple code example using a basic model.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Language Models\n",
    "A language model is at the core of text generation. It is a probabilistic model that assigns a probability to a sequence of words. The model is trained to predict the next word in a sequence given the previous words.\n",
    "\n",
    "#### Types of Language Models:\n",
    "- **Unigram, Bigram, Trigram Models**: These are basic models where a wordâ€™s probability depends on the last one or two words (n-grams).\n",
    "- **Neural Network-Based Models**: These models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Transformer models (like GPT), use deep learning to model more complex patterns.\n",
    "\n",
    "### Sequence-to-Sequence (Seq2Seq) Models\n",
    "These models are often used in tasks like translation, summarization, and dialogue generation. They consist of an encoder that processes the input text and a decoder that generates the output text.\n",
    "\n",
    "### Recurrent Neural Networks (RNNs)\n",
    "RNNs are a type of neural network specifically designed for sequential data, making them suitable for text generation. They maintain a hidden state that is updated as the model processes each word in the sequence.\n",
    "\n",
    "### Long Short-Term Memory (LSTM)\n",
    "LSTMs are a special type of RNN designed to better capture long-term dependencies in text. They help overcome the vanishing gradient problem that standard RNNs face, allowing the model to remember information over longer sequences.\n",
    "\n",
    "### Transformer Models\n",
    "The Transformer architecture, introduced in the paper \"Attention is All You Need,\" has become the standard for many NLP tasks. It uses self-attention mechanisms to weigh the importance of different words in a sequence, allowing the model to capture complex dependencies without needing sequential processing like RNNs.\n",
    "\n",
    "- **GPT (Generative Pretrained Transformer)**: GPT models, such as GPT-2 and GPT-3, are pre-trained on vast amounts of text and fine-tuned for specific tasks, making them powerful tools for text generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Text Generation Example Using RNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data\n",
    "text = \"hello world how are you\"\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = len(chars)\n",
    "hidden_size = 12\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to integers\n",
    "input_seq = [char_to_idx[ch] for ch in text[:-1]]\n",
    "target_seq = [char_to_idx[ch] for ch in text[1:]]\n",
    "\n",
    "# Convert to tensors and add batch dimension\n",
    "input_seq = torch.tensor(input_seq, dtype=torch.long).view(1, -1)\n",
    "target_seq = torch.tensor(target_seq, dtype=torch.long).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.4330837726593018\n",
      "Epoch 10, Loss: 1.6729869842529297\n",
      "Epoch 20, Loss: 1.144728660583496\n",
      "Epoch 30, Loss: 0.7640653848648071\n",
      "Epoch 40, Loss: 0.5075843334197998\n",
      "Epoch 50, Loss: 0.34087175130844116\n",
      "Epoch 60, Loss: 0.22994227707386017\n",
      "Epoch 70, Loss: 0.15846334397792816\n",
      "Epoch 80, Loss: 0.11287242919206619\n",
      "Epoch 90, Loss: 0.08266044408082962\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "\n",
    "    output, hidden = model(input_seq, hidden)\n",
    "    loss = criterion(output.view(-1, output_size), target_seq.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: hello world\n"
     ]
    }
   ],
   "source": [
    "# Text generation\n",
    "start_char = \"h\"\n",
    "input_idx = torch.tensor([char_to_idx[start_char]], dtype=torch.long).view(1, -1)\n",
    "hidden = model.init_hidden()\n",
    "\n",
    "generated_text = start_char\n",
    "for _ in range(10):  # Generate 10 characters\n",
    "    output, hidden = model(input_idx, hidden)\n",
    "    _, top_idx = output.topk(1)\n",
    "    input_idx = top_idx.squeeze().detach().view(1, -1)\n",
    "    char = idx_to_char[input_idx.item()]\n",
    "    generated_text += char\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
